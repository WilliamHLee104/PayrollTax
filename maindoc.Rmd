
---
title: "PayrollTax"
author: "William Lee"
date: "`r Sys.Date()`"
output: 
  pdf_document: default
  toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())


xfun::pkg_attach2('tidyverse', 'readxl', 'stats', 'sandwich', 'lmtest', 'ivreg', 'knitr', 
                  'lubridate', 'data.table')

proj <- dirname(rstudioapi::getSourceEditorContext()$path)

```

## Introduction

Given the importance of carefully constructing the treatment and control groups for this research project, I figured it would be a good idea to write up the procedure and discuss the results all in one document, with particular emphasis on definitions for eligibility and sample sizes. 



## Simple Data Set Statistics

```{r finaltable, echo=FALSE}

finaltable <- data.table(x = c('initial', 'round1', 'round2', 'round3', 'round4', 'round5'),
                         count = c(307380, 290825, NA, NA, NA, NA)) 

```


The Yodlee data set is indexed by unique_mem_ids which are generated by Yodlee after merging the data from all of their subcontractors. We don't exactly know how the number of unique Yodlee members translates to unique human beings or households. Naturally, Yodlee doesn't reveal how they translate their raw data into unique member ids, however we will assume that each unique member id is truly one individual (or perhaps a joint account). Since Yodlee matches across datasets from different banks and credit card companies, they are likely matching on SSNs. 

As of the August 2022 Yodlee batch, there are **56,474,837** distinct unique_mem_ids. Below, is the plot showing how the count of unique ids by month changes over time. The sample size grows considerably over time with a few minor dips presumably due to vendor contracts expiring. Throughout our analysis, we may have to make a trade off between length of time considered and sample size.


```{r usercount, echo=FALSE}

user_count <- fread(file.path(proj, 'rawdata','Count_By_Month.csv'))

user_count %>% mutate(month = as.Date(paste0(month,'01'), format = '%Y-%m%d')) %>% 
  pivot_longer(cols = -month) %>% 
  ggplot(aes(x = month, y = value)) +
  geom_line(color = 'steelblue', lwd = 1.5) +
  theme_bw() +
  facet_wrap(.~ name, scales = 'free') 

```

Since we are not able to directly observe whether or not a given user was eligible/enrolled in the Payroll Tax Deferment, we have to infer eligibility from the transaction data and refine our sample so that we have high-quality treatment and control groups. So far, we are working under the assumption that an eligible federal worker is one who:

1. Has an average Yodlee Score of at least 6.5 from September 2018 - present. (This is Yodlee's suggested value for a 'stable' user.) (show distribution of user scores)
1. Has qualifying payment observations (based on description, primary_merchant, and amount fields).
1. Has qualifying payment observations from a single employer at regular intervals (weekly, biweekly, or monthly) for 2 years before the deferal went into effect. 
1. Makes between \$2,500 - \$8666.67 per month (\$1,153.85  - \$4,000 for biweekly).We rule out individuals making too little as they are unlikely to be engaged in stable, full-time work. Since we only observe paycheck receipts and not gross salary, we further reduce the upper limits to \$7,500 and \$3,461 to cushion against payroll deductions. (Get feedback on this!)
1. Observe no more than 20% volatility between paychecks to rule out employees with varying hours worked,travel reimbursements,etc.
1. Observe no more than 30% of total inflows from other sources of income (i.e. SSI, Venmo transactions, transfers from other accounts) (Get feedback on this!)
1. Be able to link all credit card payments to credit cards in the Yodlee sample. Otherwise, we will not be observe debt-funded consumption.

A state/local employee, will match the exact same definition but with a qualifying state/local string.

### Sample Size Estimates

In this section, I try to estimate what percent of users are disqualified at each successive component of the definition. Starting with a 1% random sample of the 20220816 yodlee panel of transactions on or after '2018-01-01', I have **307,380** unique individuals, **619,830** bank accounts **334,199,899** transactions. 



```{r user_score_dist, echo = F}

user_score <- fread(file.path(proj, 'rawdata', 'User_Score_Dist.csv'))

user_score %>% 
  ggplot(aes(x = avg)) +
  geom_histogram(bins = 100, fill= 'steelblue', color = 'black') +
  labs(title = 'Average User_Score Jan 2020') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))


```
The first layer of filtering is for the user_score. Below, I plot the histogram of the average monthly user_score for January 2020 (a random month before Covid). I'm not sure how Yodlee decides these user_scores, but there are some clear clusters of user_scores. The user_score varies a lot even within a single month (average range of user_score was `r round(mean(user_score$max - user_score$min),2)`). I would love to know why user_scores vary so much in a short amount of time since things like "User History, presence of a linked account, and number of merchants" shouldn't change dramatically in a short amount of time. 

After removing all individuals with an average user_score of less than 6.5 over the sample period, we are down to `r formatC(finaltable$count[2], format = "d", big.mark = ",")` users, representing a `r round((finaltable$count[1] - finaltable$count[2])*100/finaltable$count[1],2)` percent reduction in the sample. Perhaps it's worth exploring a higher threshold, especially since the main analysis may involve daily time coefficients. 



```{r filter2, echo = F}

payroll <- fread(file.path(proj, 'rawdata', 'Find_all_Qualifying__Payroll_Transactions.csv')) %>% 
  filter(is_duplicate == 0) %>% 
  mutate(optimized_transaction_date = ymd(optimized_transaction_date),
         yr_mon = lubridate::round_date(optimized_transaction_date, unit = 'month'),
         yr_week = paste0(lubridate::year(optimized_transaction_date), lubridate::epiweek(optimized_transaction_date)) ,
         yr_week_start = lubridate::round_date(optimized_transaction_date, unit = 'week'))

finaltable$count[3] <- uniqueN(payroll$unique_mem_id)

simple_count <- payroll %>% 
  group_by(fed) %>% 
  summarize(n = uniqueN(unique_mem_id)) 

```

The second layer of filtering is to sort each individual into federal/state/local based on the description and primary merchant. The full .sql used is included in the appendix, but uses all of the patterns we have found up to this point. After filtering, we are left with `r formatC(finaltable$count[3], format = "d", big.mark = ",")` individuals which represents a `r round((finaltable$count[2] - finaltable$count[3])*100/finaltable$count[2],2)` percent reduction in eligible individuals. `r simple_count$n[which(simple_count$fed == 'federal')]` are federal, `r simple_count$n[which(simple_count$fed == 'state')]` are state, and `r simple_count$n[which(simple_count$fed == 'local')]` are local.

I tried to match these figures with outside estimates of the federal/state/local workforce. Despite the lack of a single definition for federal worker, this [Brookings piece](https://www.brookings.edu/policy2020/votervital/public-service-and-the-federal-government/) has similar estimates to our data (~15% in some sort of federal/state/local government, ~6% in federal).

```{r filter3, echo = F}


payrollf3 <- payroll %>% 
  arrange(unique_mem_id, optimized_transaction_date) %>%
  group_by(unique_mem_id) %>% 
  mutate(paycheck_gap = c(NA,diff(optimized_transaction_date)),
         median_paycheck_gap = median(paycheck_gap[paycheck_gap != 0], na.rm = T), 
         pay_freq = case_when(median_paycheck_gap >= 6 & median_paycheck_gap <= 8 ~ "weekly",
                              median_paycheck_gap >= 13 & median_paycheck_gap <= 15 ~ "biweekly",
                              median_paycheck_gap >= 28 & median_paycheck_gap <= 32 ~ "monthly",
                              TRUE ~ 'other')) %>% 
  filter(pay_freq != 'other')

payrollf3 <- payrollf3 %>% 
  group_by(unique_mem_id) %>% 
  mutate(n_employers = uniqueN(primary_merchant_name), nobs = n()) %>% 
  filter(n_employers == 1 ) %>% 
  filter(nobs > (ymd("2022-06-01") - ymd("2018-08-01"))/median_paycheck_gap)

finaltable$count[4] <- uniqueN(payrollf3$unique_mem_id)
  

```

After the third round of filtering, we are left with `r formatC(finaltable$count[4], format = "d", big.mark = ",")` individuals which represents a `r round((finaltable$count[3] - finaltable$count[4])*100/finaltable$count[3],2)` percent reduction in eligible individuals. Most of the reduction is coming from the restriction on having four years of consistent paychecks from a single employer. Should we relax the definition a bit on the pre-period?

I'd be interested to hear your thoughts on whether restricting our analysis to people who do not switch jobs or take side-jobs limits the external validity of our sample. On one hand, a refined sample will allow us to cleanly estimate the effects of the deferral on individual, but on the other hand, we will not be to make strong statements about the overall impact of the program. 


After the fourth layer of filtering which classes individuals by income thresholds, we are left with the following number of unique_id_mem's. The proportions seem about right, given that most government employees that have regular paychecks should make between 30,000 - 104,000 USD per year. 

```{r filter4, echo = F}

monthly_upperlimit <- 8666.67
monthly_lowerlimit <- 2500

biweekly_upperlimit <- 4000
biweekly_lowerlimit <- 1153.85

payrollf4 <- payrollf3 %>% 
  group_by(unique_mem_id) %>% 
  mutate(class1 = sum((yr_week_start == '2020-07-26'), na.rm = T) >= 1,
         class2 = sum((yr_week_start == '2020-08-02'), na.rm = T) >= 1, 
         class3 = sum((yr_week_start == '2020-08-09'), na.rm = T) >= 1,
         class4= sum((yr_week_start == '2020-08-16'), na.rm = T) >= 1,
         class5 = sum((yr_week_start == '2020-08-23'), na.rm = T) >= 1,
         class6 = sum((yr_week_start == '2020-08-30'), na.rm = T) >= 1,
         class = case_when(class1 ~ 'class1', class2 ~'class2', class3 ~ 'class3',
                            class4 ~ 'class4', class5 ~ 'class5', TRUE ~ 'other'),
         cohort = paste0(pay_freq, "-", class)) %>%
  filter(cohort %in% c('biweekly-class1', 'biweekly-class2', 'monthly-class1'))  
# filter out people who are off cycle/ have no observations in Aug 2020


elig <- payrollf4 %>% 
  mutate(last_paycheck_week = case_when(cohort == 'monthly-class1' ~ ymd("2020-08-30"),
                                        cohort == 'biweekly-class1' ~ ymd('2020-08-23'),
                                        cohort == 'biweekly-class2' ~ ymd('2020-08-30'))) %>% 
  filter(yr_week_start == last_paycheck_week) %>% 
  group_by(unique_mem_id) %>% 
  mutate(last_paycheck_amount = sum(amount, na.rm = T)) %>% #some paychecks come in two chunks on the same day
  mutate(elig = case_when(cohort == 'monthly-class1' & last_paycheck_amount >= monthly_lowerlimit & last_paycheck_amount <= monthly_upperlimit ~ 'elig',
                          cohort == 'biweekly-class1' & last_paycheck_amount >= biweekly_lowerlimit & last_paycheck_amount <= biweekly_upperlimit ~ 'elig', 
                          cohort == 'biweekly-class2' & last_paycheck_amount >= biweekly_lowerlimit & last_paycheck_amount <= biweekly_upperlimit ~ 'elig',
                          TRUE ~ 'inelig')) %>% 
  select(unique_mem_id, elig, last_paycheck_amount, last_paycheck_week)

payrollf4 %>% 
  inner_join(elig, by = 'unique_mem_id') %>% 
  group_by(elig, fed) %>% 
  summarize(n = uniqueN(unique_mem_id)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = fed, values_from = n)

```


Below is a histogram of percentage volatility (std(paycheck_amt)*100/mean(paycheck_amt)). The threshold of 20% is entirely arbitrary, so I wanted to show the general distribution. I need to examine these individuals more to see why they have such volatility, but I think it's better to exclude them for now until we can better understand the trends. I'm guessing the common causes for paycheck volatility are pay raises, bonuses (are these common for government employees), changes in withholding, reimbursements, etc.


```{r f5p1, echo = F}
payrollf4 %>% 
  inner_join(elig, by = 'unique_mem_id') %>% 
  group_by(unique_mem_id) %>% 
  summarize(paycheck_vol = sd(amount, na.rm = T)*100/mean(amount, na.rm = T)) %>% 
  ggplot(aes(x = paycheck_vol)) +
  geom_histogram(bins = 50, color= 'black', fill = 'steelblue') +
  xlim(c(-1,51))
```

After the fifth round of filtering which removes individuals with high volatility in paychecks, we are left with the following number of individuals in each category. 


```{r f5p2, echo = F}
payrollf5 <- payrollf4 %>% 
  inner_join(elig, by = 'unique_mem_id') %>% 
  group_by(unique_mem_id) %>% 
  mutate(paycheck_vol = sd(amount, na.rm = T)*100/mean(amount, na.rm = T)) %>% 
  filter(paycheck_vol <= 20)

payrollf5 %>% 
  group_by(elig, fed) %>% 
  summarize(n = uniqueN(unique_mem_id)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = fed, values_from = n)

```

There are two more layers of filter: one for ruling out other sources of income and another for ensuring linkability of Yodlee data. I have some ideas on how to do this efficiently, but I'd like your feedback first. 

## Key graph

Here is the most important piece of the whole exercise. After all of this cleaning, we need to see if the deferral is visible in a simple weekly aggregate plot, and I think it looks pretty good. The trend is clearly visible for eligible federal employees but absent for state and local employees. However, I'm concerned to see a small increase in the ineligible (due to the salary cutoff) federal employees during the payroll period. 

```{r moneyshot, echo = FALSE, message = FALSE, warning = FALSE}

moneyshot <- function(x, timeframe){
  x %>% 
  group_by(unique_mem_id, {{timeframe}}, fed) %>%
  summarize(monthly_pay = sum(amount, na.rm = T)) %>%
  group_by(unique_mem_id, fed) %>% 
  mutate(scaled_amount = monthly_pay/mean(monthly_pay)) %>% 
  group_by({{timeframe}}, fed) %>% 
  summarize(scaled_avg_pay = mean(scaled_amount)) %>%
  ggplot(aes(x = {{timeframe}}, y = scaled_avg_pay, group = fed, color = fed)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ x, se = F) +
  geom_vline(xintercept = ymd('2020-09-01'), color = 'black') +
  geom_vline(xintercept = ymd('2021-01-01'), color = 'black') +
  theme_bw() +
  theme(legend.position = 'bottom') +
  facet_wrap(.~ fed, ncol = 1)
}

moneyshot(payrollf5 %>% filter(elig == "elig"), yr_week_start)

moneyshot(payrollf5 %>% filter(elig == "inelig"), yr_week_start)
```


## Appendix


```{r 2, file = 'C:/Users/willi/Documents/PayrollTax/user_count.sql', engine = 'sql', eval = FALSE}


```





